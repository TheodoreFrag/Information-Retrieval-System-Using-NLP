{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval System Using NLP\n",
    "This notebook demonstrates the creation of a search engine, including text preprocessing, indexing, and document retrieval with ranking methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection\n",
    "We collect Wikipedia articles using either a web crawler or a pre-existing dataset. Below is the code for collecting and saving the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def scrape_wikipedia(url, max_articles=10):\n",
    "    articles = {}\n",
    "    visited_urls = set()\n",
    "    to_visit = [url]\n",
    "\n",
    "    while to_visit and len(articles) < max_articles:\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        response = requests.get(current_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract title and content\n",
    "        title = soup.find('h1').text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = \" \".join([p.text for p in paragraphs])\n",
    "\n",
    "        # Save the article\n",
    "        articles[title] = content\n",
    "        visited_urls.add(current_url)\n",
    "\n",
    "        # Find additional links\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and ':' not in href:\n",
    "                full_url = f\"https://en.wikipedia.org{href}\"\n",
    "                if full_url not in visited_urls:\n",
    "                    to_visit.append(full_url)\n",
    "\n",
    "    # Save articles to JSON\n",
    "    with open('wikipedia_articles.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "scrape_wikipedia(\"https://en.wikipedia.org/wiki/Natural_language_processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Text Preprocessing\n",
    "Text preprocessing involves cleaning and tokenizing the text, removing stopwords, and applying lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import json\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Lowercasing and removing special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    # Tokenization\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def preprocess_articles(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    preprocessed_articles = {}\n",
    "    for title, content in articles.items():\n",
    "        preprocessed_articles[title] = preprocess_text(content)\n",
    "\n",
    "    with open('preprocessed_articles.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(preprocessed_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "preprocess_articles('wikipedia_articles.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Indexing\n",
    "We create an inverted index for efficient term-document mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_inverted_index(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    inverted_index = {}\n",
    "    for doc_id, tokens in articles.items():\n",
    "        for token in tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = []\n",
    "            inverted_index[token].append(doc_id)\n",
    "\n",
    "    with open('inverted_index.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(inverted_index, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "build_inverted_index('preprocessed_articles.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Query Processing\n",
    "Users can input queries to retrieve documents using Boolean Retrieval, TF-IDF, or BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json\n",
    "\n",
    "def search_engine():\n",
    "    with open('preprocessed_articles.json', 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    corpus = [\" \".join(tokens) for tokens in articles.values()]\n",
    "    titles = list(articles.keys())\n",
    "\n",
    "    # TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # BM25 Model\n",
    "    bm25 = BM25Okapi([tokens for tokens in articles.values()])\n",
    "\n",
    "    print(\"Enter your query (or 'exit' to quit):\")\n",
    "    while True:\n",
    "        query = input(\"Query: \").lower()\n",
    "        if query == 'exit':\n",
    "            break\n",
    "\n",
    "        print(\"Select ranking method:\")\n",
    "        print(\"1. Boolean Search\")\n",
    "        print(\"2. TF-IDF Search\")\n",
    "        print(\"3. BM25 Search\")\n",
    "        method = int(input(\"Choice: \"))\n",
    "\n",
    "        if method == 1:\n",
    "            query_tokens = query.split()\n",
    "            results = [doc for doc in titles if all(token in articles[doc] for token in query_tokens)]\n",
    "            print(f\"Boolean Search Results: {results}\")\n",
    "\n",
    "        elif method == 2:\n",
    "            query_vector = vectorizer.transform([query])\n",
    "            scores = (tfidf_matrix @ query_vector.T).toarray().flatten()\n",
    "            ranked_indices = np.argsort(-scores)\n",
    "            print(\"TF-IDF Ranked Results:\")\n",
    "            for idx in ranked_indices[:5]:\n",
    "                print(f\"{titles[idx]} (Score: {scores[idx]:.4f})\")\n",
    "\n",
    "        elif method == 3:\n",
    "            query_tokens = query.split()\n",
    "            scores = bm25.get_scores(query_tokens)\n",
    "            ranked_indices = np.argsort(-scores)\n",
    "            print(\"BM25 Ranked Results:\")\n",
    "            for idx in ranked_indices[:5]:\n",
    "                print(f\"{titles[idx]} (Score: {scores[idx]:.4f})\")\n",
    "\n",
    "search_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation\n",
    "We evaluate the system using metrics like Precision, Recall, and NDCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_system(true_relevant_docs, retrieved_docs):\n",
    "    true_positive = len(set(true_relevant_docs) & set(retrieved_docs))\n",
    "    false_positive = len(set(retrieved_docs) - set(true_relevant_docs))\n",
    "    false_negative = len(set(true_relevant_docs) - set(retrieved_docs))\n",
    "\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Example evaluation\n",
    "true_relevant_docs = ['Article 1', 'Article 2']\n",
    "retrieved_docs = ['Article 2', 'Article 3']\n",
    "evaluate_system(true_relevant_docs, retrieved_docs)\n",
    "\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Example: Evaluating NDCG\n",
    "true_relevance = [[1, 0, 0, 0, 1]]  # Binary relevance\n",
    "predicted_scores = [[0.9, 0.7, 0.3, 0.2, 0.8]]  # Model scores\n",
    "ndcg = ndcg_score(true_relevance, predicted_scores)\n",
    "print(f\"NDCG Score: {ndcg:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
