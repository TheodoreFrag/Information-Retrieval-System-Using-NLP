{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval System Using NLP\n",
    "This notebook demonstrates the creation of a search engine, including text preprocessing, indexing, and document retrieval with ranking methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection\n",
    "We collect Wikipedia articles using either a web crawler or a pre-existing dataset. Below is the code for collecting and saving the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "def scrape_wikipedia(url, max_articles=10):\n",
    "    articles = {}\n",
    "    visited_urls = set()\n",
    "    to_visit = [url]\n",
    "\n",
    "    while to_visit and len(articles) < max_articles:\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        response = requests.get(current_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract title and content\n",
    "        title = soup.find('h1').text\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = \" \".join([p.text for p in paragraphs])\n",
    "\n",
    "        # Save the article\n",
    "        articles[title] = content\n",
    "        visited_urls.add(current_url)\n",
    "\n",
    "        # Find additional links\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('/wiki/') and ':' not in href:\n",
    "                full_url = f\"https://en.wikipedia.org{href}\"\n",
    "                if full_url not in visited_urls:\n",
    "                    to_visit.append(full_url)\n",
    "\n",
    "    # Save articles to JSON\n",
    "    with open('wikipedia_articles.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "scrape_wikipedia(\"https://en.wikipedia.org/wiki/Natural_language_processing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Text Preprocessing\n",
    "Text preprocessing involves cleaning and tokenizing the text, removing stopwords, and applying lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/teo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/teo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import json\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Lowercasing and removing special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    # Tokenization\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def preprocess_articles(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    preprocessed_articles = {}\n",
    "    for title, content in articles.items():\n",
    "        preprocessed_articles[title] = preprocess_text(content)\n",
    "\n",
    "    with open('preprocessed_articles.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(preprocessed_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "preprocess_articles('wikipedia_articles.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Indexing\n",
    "We create an inverted index for efficient term-document mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def build_inverted_index(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    inverted_index = {}\n",
    "    for doc_id, tokens in articles.items():\n",
    "        for token in tokens:\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = []\n",
    "            inverted_index[token].append(doc_id)\n",
    "\n",
    "    with open('inverted_index.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(inverted_index, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "build_inverted_index('preprocessed_articles.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Query Processing\n",
    "Users can input queries to retrieve documents using Boolean Retrieval, TF-IDF, or BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query (or 'exit' to quit):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m ranked_indices[:\u001b[38;5;241m5\u001b[39m]:\n\u001b[1;32m     51\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitles[idx]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores[idx]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m \u001b[43msearch_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m, in \u001b[0;36msearch_engine\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter your query (or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to quit):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuery: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import json\n",
    "\n",
    "def search_engine():\n",
    "    with open('preprocessed_articles.json', 'r', encoding='utf-8') as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    corpus = [\" \".join(tokens) for tokens in articles.values()]\n",
    "    titles = list(articles.keys())\n",
    "\n",
    "    # TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # BM25 Model\n",
    "    bm25 = BM25Okapi([tokens for tokens in articles.values()])\n",
    "\n",
    "    print(\"Enter your query (or 'exit' to quit):\")\n",
    "    while True:\n",
    "        query = input(\"Query: \").lower()\n",
    "        if query == 'exit':\n",
    "            break\n",
    "\n",
    "        print(\"Select ranking method:\")\n",
    "        print(\"1. Boolean Search\")\n",
    "        print(\"2. TF-IDF Search\")\n",
    "        print(\"3. BM25 Search\")\n",
    "        method = int(input(\"Choice: \"))\n",
    "\n",
    "        if method == 1:\n",
    "            query_tokens = query.split()\n",
    "            results = [doc for doc in titles if all(token in articles[doc] for token in query_tokens)]\n",
    "            print(f\"Boolean Search Results: {results}\")\n",
    "\n",
    "        elif method == 2:\n",
    "            query_vector = vectorizer.transform([query])\n",
    "            scores = (tfidf_matrix @ query_vector.T).toarray().flatten()\n",
    "            ranked_indices = np.argsort(-scores)\n",
    "            print(\"TF-IDF Ranked Results:\")\n",
    "            for idx in ranked_indices[:5]:\n",
    "                print(f\"{titles[idx]} (Score: {scores[idx]:.4f})\")\n",
    "\n",
    "        elif method == 3:\n",
    "            query_tokens = query.split()\n",
    "            scores = bm25.get_scores(query_tokens)\n",
    "            ranked_indices = np.argsort(-scores)\n",
    "            print(\"BM25 Ranked Results:\")\n",
    "            for idx in ranked_indices[:5]:\n",
    "                print(f\"{titles[idx]} (Score: {scores[idx]:.4f})\")\n",
    "\n",
    "search_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluation\n",
    "We evaluate the system using metrics like Precision, Recall, and NDCG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5000\n",
      "Recall: 0.5000\n",
      "F1-Score: 0.5000\n",
      "NDCG Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_system(true_relevant_docs, retrieved_docs):\n",
    "    true_positive = len(set(true_relevant_docs) & set(retrieved_docs))\n",
    "    false_positive = len(set(retrieved_docs) - set(true_relevant_docs))\n",
    "    false_negative = len(set(true_relevant_docs) - set(retrieved_docs))\n",
    "\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Example evaluation\n",
    "true_relevant_docs = ['Article 1', 'Article 2']\n",
    "retrieved_docs = ['Article 2', 'Article 3']\n",
    "evaluate_system(true_relevant_docs, retrieved_docs)\n",
    "\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Example: Evaluating NDCG\n",
    "true_relevance = [[1, 0, 0, 0, 1]]  # Binary relevance\n",
    "predicted_scores = [[0.9, 0.7, 0.3, 0.2, 0.8]]  # Model scores\n",
    "ndcg = ndcg_score(true_relevance, predicted_scores)\n",
    "print(f\"NDCG Score: {ndcg:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
